{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475134323744_-72191159","id":"20160929-073203_220333229","dateCreated":"2016-09-29T07:32:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:4746","text":"%pyspark\nfrom pyspark.mllib.feature import StandardScaler\nfrom pyspark.mllib.regression import LinearRegressionWithSGD\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark import SparkContext\nimport numpy as np\n\n#import boston-house.csv data from Hadoop\nfileRdd = sc.textFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/boston-house.csv')\n\n#remove header and split the value in each column column \nheader = fileRdd.first()\nheaderless = fileRdd.filter(lambda line: line != header)\nheaderless_splitted = headerless.map(lambda line: line.split(\",\"))\n\n#extract the features and prediction value\nfeatures = headerless_splitted.map(lambda row: row[:-1])\nmedv = headerless_splitted.map(lambda row: row[-1])\n\n#scale and transform the features\nstandardscaler = StandardScaler(withMean=True, withStd=True)\nfeature_scaled = standardscaler.fit(features)\nfeatures_transformed = feature_scaled.transform(features)\n\n#combine features and prediction value in to transformedData\ntransformedData = medv.zip(features_transformed)\n\n#create labeled point format, which is a tuple of the response value and a vector of predictors, to satisfy Mllib requirement\ntransformedData = transformedData.map(lambda row: LabeledPoint(row[0],[row[1]]))\n\n#train the transformedData\nmodel = LinearRegressionWithSGD.train(transformedData, intercept=True)\n\n#import the verification data, scale, and transform to be used for verifying prediction (follow the same steps)\nverificationRdd = sc.textFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/verification.csv')\nheader_ver = verificationRdd.first()\nheaderless_ver = verificationRdd.filter(lambda line: line != header_ver)\nheaderless_ver_splitted = headerless_ver.map(lambda line: line.split(\",\"))\nver_transformed = feature_scaled.transform(headerless_ver_splitted)\n\n#predict the verification data using the trained model\nverify_predictions = model.predict(ver_transformed)\nprint(verify_predictions.take(3))\n\n#save the result to Hadoop\nverify_predictions.saveAsTextFile('hdfs://sandbox.hortonworks.com/tmp/boston-data/predicted_results')","dateUpdated":"2016-09-29T17:01:52+0000","dateFinished":"2016-09-29T17:04:07+0000","dateStarted":"2016-09-29T17:01:55+0000","result":{"code":"SUCCESS","type":"TEXT","msg":"[17.421431247081873, 21.041183220138773, 20.464012215912089]\n"},"focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475134337944_1786176677","id":"20160929-073217_1601723170","dateCreated":"2016-09-29T07:32:17+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4823"}],"name":"Group-11-Boston-House","id":"2BZTJC765","angularObjects":{"2BW4S5NJ4:shared_process":[],"2BY1Y2HJG:shared_process":[],"2BVYH21ZJ:shared_process":[],"2BV6637D6:shared_process":[],"2BX5CPDSX:shared_process":[],"2BWHGGFZJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}